%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EXAMPLES:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EXAMPLE FIGURE:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[ht]
%     \vskip 0.2in
%     \begin{center}
%         \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
%         \caption{Historical locations and number of accepted papers for International
%             Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
%             Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
%             produced, the number of accepted papers for ICML 2008 was unknown and instead
%             estimated.}
%         \label{icml-historical}
%     \end{center}
%     \vskip -0.2in
% \end{figure}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EXAMPLE ALGORITHM:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{algorithm}[tb]
%     \caption{Bubble Sort}
%     \label{alg:example}
%     \begin{algorithmic}
%         \STATE {\bfseries Input:} data $x_i$, size $m$
%         \REPEAT
%         \STATE Initialize $noChange = true$.
%         \FOR{$i=1$ {\bfseries to} $m-1$}
%         \IF{$x_i > x_{i+1}$}
%         \STATE Swap $x_i$ and $x_{i+1}$
%         \STATE $noChange = false$
%         \ENDIF
%         \ENDFOR
%         \UNTIL{$noChange$ is $true$}
%     \end{algorithmic}
% \end{algorithm}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EXAMPLE TABLE:
% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}[t]
%     \caption{Classification accuracies for naive Bayes and flexible
%         Bayes on various data sets.}
%     \label{sample-table}
%     \vskip 0.15in
%     \begin{center}
%         \begin{small}
%             \begin{sc}
%                 \begin{tabular}{lcccr}
%                     \toprule
%                     Data set  & Naive         & Flexible      & Better?  \\
%                     \midrule
%                     Breast    & 95.9$\pm$ 0.2 & 96.7$\pm$ 0.2 & $\surd$  \\
%                     Cleveland & 83.3$\pm$ 0.6 & 80.0$\pm$ 0.6 & $\times$ \\
%                     Glass2    & 61.9$\pm$ 1.4 & 83.8$\pm$ 0.7 & $\surd$  \\
%                     Credit    & 74.8$\pm$ 0.5 & 78.3$\pm$ 0.6 &          \\
%                     Horse     & 73.3$\pm$ 0.9 & 69.7$\pm$ 1.0 & $\times$ \\
%                     Meta      & 67.1$\pm$ 0.6 & 76.5$\pm$ 0.5 & $\surd$  \\
%                     Pima      & 75.1$\pm$ 0.6 & 73.9$\pm$ 0.5 &          \\
%                     Vehicle   & 44.9$\pm$ 0.6 & 61.5$\pm$ 0.4 & $\surd$  \\
%                     \bottomrule
%                 \end{tabular}
%             \end{sc}
%         \end{small}
%     \end{center}
%     \vskip -0.1in
% \end{table}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PROJECT OVERVIEW:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The goal of this project is for you to further explore a certain subject that we covered
% in class or a new subject related to it (based on some previous work). This is an open
% project in a sense that you will propose the problem you would like to work on and the
% solution you are planning use. Your main goal should be to explore the characteristics of
% your problem/solution under the reinforcement learning framework
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% REPORT INSTRUCTIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1. Introduction(5%)
%In this section you should discuss the question you are planning to investigate. Make
%sure to describe the problem clearly, and how you are planning to solve it. Also, clarify
%what your new contribution is (as opposed to previous works).
%2. Previous Work(5%)
%Discuss the previous work you are using as a starting point/reference. If you are
%working on a different domain include work that works on the same or similar domain.
%If you are using a new method, discuss other works which use similar methods. Note
%that you can use both academic papers in addition to online articles. Make sure to cite
%all sources.
%3. Domain Problem(5%)
%Describe the problem you are working on including the rules, goals, etc. Assume that
%the reader has not been introduced to this domain problem previously. Then describe
%how you frame it as an MDP, including: state space, action space and reward structure.
%If you are examining different options describe them all. Feel free to add diagrams to
%clarify.
%3
%4. Reinforcement Learning Methods(5%)
%Describe the reinforcement method/methods you are planning to use in this work. This
%should give the background for the methods. If you are comparing multiple ones, make
%sure to describe them all. Feel free to add diagrams and equations to clarify.
%5. Code Design(5%)
%Describe the general structure of your code including functions, classes, and data struc-
%tures used. If you are using code written by someone else make sure to cite it and
%emphasize the parts that you have written yourself.
%6. Results Analysis(40%)
%Present the results which answer your hypothesis questions. These should include dif-
%ferent graphs which clarify the answer and examine different aspects of your solution.
%Make sure to carefully describe how your experiments were conducted (how many runs,
%how it was initialized, etc. This section is the most important, and will be evaluated
%both on presentation and correctness.
%7. Conclusion(5%)
%What have you achieved in this project? What have you learned?
%An appendix describing which part of the project each team member was in charge of.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{GPT2PPO: Auto Regressive Proximal Policy Optimization\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Andrei Cozma}
\IEEEauthorblockA{\textit{Department of Electrical Engineering \& Computer Science} \\
\textit{University of Tennessee}\\
Knoxville, United States \\
acozma@vols.utk.edu}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Hunter Price}
\IEEEauthorblockA{\textit{Department of Electrical Engineering \& Computer Science} \\
\textit{University of Tennessee}\\
Knoxville, United States \\
hprice7@vols.utk.edu}
}

\maketitle


\begin{abstract}
    This document is a model and instructions for \LaTeX.
    This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
    or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
    Reinforcement Learning, PPO, GPT2
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Introduction(5%)
%In this section you should discuss the question you are planning to investigate. Make
%sure to describe the problem clearly, and how you are planning to solve it. Also, clarify
%what your new contribution is (as opposed to previous works).

\section{Introduction}
% \label{sec:introduction}
In this project we explore the use of transformers in the context of Reinforcement Learning. The majority of theoretical works 
assume that problems follow a Markovian process, which is not always the case. Some problems need the contxt of previous states and actions
to make an informed decision on the next decision. As a result, we propose an addition to the basic Proximal Policy Optimization (PPO) algorithm
by using the Generative Pre-trained Transformer 2 (GPT2) model as the encoder for the critic network. This will allow the critic network to
take into account the context of previous states and actions as well as apply attention to past states and actions that may be important. 
We will test this model on the LunarLander-v2 and Acrobot-v1 OpenAi Gym environments with discrete action spaces and compare it to the original PPO algorithm. 
Additionally we will test the model on BipedalWalker-v3 with continuous action spaces and compare it to the original PPO algorithm.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Previous Work(5%)
%Discuss the previous work you are using as a starting point/reference. If you are
%working on a different domain include work that works on the same or similar domain.
%If you are using a new method, discuss other works which use similar methods. Note
%that you can use both academic papers in addition to online articles. Make sure to cite
%all sources.

\section{Previous Work}
% \label{sec:previous-work}

% PPO citation - \cite{schulman2017proximal}
% Decision Transformer citation - \cite{chen2021decision}
% GPT2 citation - \cite{radford2019language}
% Wandb citation - \cite{wandb}
% Hugging face transformers citation - \cite{wolf-etal-2020-transformers}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Domain Problem(5%)
%Describe the problem you are working on including the rules, goals, etc. Assume that
%the reader has not been introduced to this domain problem previously. Then describe
%how you frame it as an MDP, including: state space, action space and reward structure.
%If you are examining different options describe them all. Feel free to add diagrams to
%clarify.
\section{Background}
% \label{sec:background}

All of the environments used in this project are from the OpenAI Gym library\cite{brockman2016openai}. The environments are described below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lunar Lander}

The Lunar Lander environment is a rocket trajectory optimization  
problem\footnote{OpenAI Gym Lunar Lander: \url{https://www.gymlibrary.dev/environments/box2d/lunar_lander}}
shown in Figure \ref{fig:lunar-lander-env}. The goal is to actuate the lander to the landing pad at coordinates (0,0) without crashing.
OpenAI Gym offers two versions of the environment: discrete or continuous.
In this work we only use the discrete version. 
The state space is a 8-dimensional vector containing the x and y positional coordinates of the agent, its x and y linear velocities,
its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.
The action space is a single discrete scalar with values ranging from 0 to 3. The values corresponspond to the following actions: 
do nothing, fire left orientation engine, fire main engine, fire right orientation engine.
The reward structure contains both positive and negative rewards. If the lander moves away from the landing pad, it gains a negative reward.
If the lander crashes, it receives an -100 reward. If it comes to rest, it receives an +100 reward. Each leg with ground contact is +10 points.
Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame. Firing the side engine is -0.03 points each frame.
The landers initial state is at the top center of the environment with a random intial force applied to its center. 
The episode ends if the lander crashes, goes outside of the viewport, or comes to a resting position.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/lunar-lander-env.png}}
    \caption{The Lunar Lander environment.}
    \label{fig:lunar-lander-env}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Acrobot}

Open AI Gym's implementation of the Acrobot 
environment\footnote{OpenAI Gym Acrobot: \url{https://www.gymlibrary.dev/environments/classic_control/acrobot}}
is based off the work of Sutton and Barto\cite{sutton2018reinforcement} shown in Figure \ref{fig:acrobot-env}.
The environment is a 2-link pendulum with only the second joint actuated with a discrete action space. The goal is to swing the end of the pendulum up to a given height. 
The state space is a 6-dimensional vector containing the sin and cos of the two joint angles and the joint angular velocities.
The action space is a single discrete scalar with values ranging from 0 to 5. The values corresponspond to the following actions:
apply +1, 0, or -1 torque to the actuated joint. 
The reward structure contains only negative rewards. At each timestep the agent receives a reward of -1 for each step that does not reach the goal.
If the goal is reached the agent receives a reward of 0. The episode ends if the agent reaches the goal height or if the episode exceeds the maximum number of timesteps.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/acrobot-env.png}}
    \caption{The Acrobot environment.}
    \label{fig:acrobot-env}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bipedal Walker}
%. There are no coordinates in the state vector.
The Bipedal Walker environment is a 2D simulation of a bipedal walker robot with 
4-joints\footnote{OpenAI Gym Acrobot: \url{https://www.gymlibrary.dev/environments/box2d/bipedal_walker}} 
shown in Figure \ref{bipedal-walker-env}. The goal is to keep the walker upright for as long as possible.
The state space is a 24-dimensional vector containing: 
the hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, 
legs contact with ground, and 10 lidar rangefinder measurements. Notably, there are no coordinates given in the state vector.
The action space is a 4 dimensional vector containing continuous values of each joints motor speed between -1 and 1.
The reward structure contains both positive and negative rewards. A positive reward is given for moving forward. If the agent falls it receives a reward of -100.
Applying motor torque costs a small amount of reward.
The agent's intial state is standing at the left of the terrain with the hull horizontal, with both legs in the same position with a slight knee angle. 
The episode terminates if the agent's hull makes contact with the ground or if the agent reaches the end of the terrain length.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/bipedal-walker-env.png}}
    \caption{The Bipedal Walker environment.}
    \label{bipedal-walker-env}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
% \label{sec:methodology}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Reinforcement Learning Methods(5%)
%Describe the reinforcement method/methods you are planning to use in this work. This
%should give the background for the methods. If you are comparing multiple ones, make
%sure to describe them all. Feel free to add diagrams and equations to clarify.

\subsection{Reinforcement Learning Methods}
% \label{subsec:reinf-learning-methods}
% PPO, Transformers, GPT

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Code Design(5%)
%Describe the general structure of your code including functions, classes, and data struc-
%tures used. If you are using code written by someone else make sure to cite it and
%emphasize the parts that you have written yourself.

\subsection{Code Design}
% \label{subsec:design}

%Regarding the code structure employed, we have two main Python files called \texttt{Q\_Learning.py} and \texttt{runEpisode.py}.
%
%The other files are found in the \texttt{rlenv} and \texttt{rllib} folders. The \texttt{rlenv} folder contains the code for the game environment in \texttt{pongclass.py}, while the \texttt{rllib} folder contains \texttt{agent. py}, which contains the code for the Q-Learning and SARSA agents, as well as \texttt{utils.py}, which contains utility functions for parsing command line arguments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{main\_train.py}
% \label{subsec:train.py}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{main\_eval.py}
% \label{subsec:eval.py}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Results Analysis(40%)
%Present the results which answer your hypothesis questions. These should include dif-
%ferent graphs which clarify the answer and examine different aspects of your solution.
%Make sure to carefully describe how your experiments were conducted (how many runs,
%how it was initialized, etc. This section is the most important, and will be evaluated
%both on presentation and correctness.


\section{Results}
% \label{sec:results}


%\begin{figure}[htb]
%    \centering
%    \includegraphics[width=\columnwidth]{./img/q-vs-s-running-offline-reward.png}
%    \caption{Q-learning vs. SARSA average offline reward.}
%    \label{q-vs-s-running-offline-reward}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Conclusion(5%)
%What have you achieved in this project? What have you learned?

\section{Conclusion}
% \label{sec:conclusion}
% \lipsum[2-4]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References
\bibliographystyle{IEEEtran}
\bibliography{main}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% An appendix describing which part of the project each team member was in charge of.
\appendix
\subsection{Team Member Contributions}

\textbf{Andrei Cozma}. text here.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Hunter Price}. text here.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
