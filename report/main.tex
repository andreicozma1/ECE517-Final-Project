%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EXAMPLES:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EXAMPLE FIGURE:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[ht]
%     \vskip 0.2in
%     \begin{center}
%         \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
%         \caption{Historical locations and number of accepted papers for International
%             Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
%             Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
%             produced, the number of accepted papers for ICML 2008 was unknown and instead
%             estimated.}
%         \label{icml-historical}
%     \end{center}
%     \vskip -0.2in
% \end{figure}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EXAMPLE ALGORITHM:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{algorithm}[tb]
%     \caption{Bubble Sort}
%     \label{alg:example}
%     \begin{algorithmic}
%         \STATE {\bfseries Input:} data $x_i$, size $m$
%         \REPEAT
%         \STATE Initialize $noChange = true$.
%         \FOR{$i=1$ {\bfseries to} $m-1$}
%         \IF{$x_i > x_{i+1}$}
%         \STATE Swap $x_i$ and $x_{i+1}$
%         \STATE $noChange = false$
%         \ENDIF
%         \ENDFOR
%         \UNTIL{$noChange$ is $true$}
%     \end{algorithmic}
% \end{algorithm}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EXAMPLE TABLE:
% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}[t]
%     \caption{Classification accuracies for naive Bayes and flexible
%         Bayes on various data sets.}
%     \label{sample-table}
%     \vskip 0.15in
%     \begin{center}
%         \begin{small}
%             \begin{sc}
%                 \begin{tabular}{lcccr}
%                     \toprule
%                     Data set  & Naive         & Flexible      & Better?  \\
%                     \midrule
%                     Breast    & 95.9$\pm$ 0.2 & 96.7$\pm$ 0.2 & $\surd$  \\
%                     Cleveland & 83.3$\pm$ 0.6 & 80.0$\pm$ 0.6 & $\times$ \\
%                     Glass2    & 61.9$\pm$ 1.4 & 83.8$\pm$ 0.7 & $\surd$  \\
%                     Credit    & 74.8$\pm$ 0.5 & 78.3$\pm$ 0.6 &          \\
%                     Horse     & 73.3$\pm$ 0.9 & 69.7$\pm$ 1.0 & $\times$ \\
%                     Meta      & 67.1$\pm$ 0.6 & 76.5$\pm$ 0.5 & $\surd$  \\
%                     Pima      & 75.1$\pm$ 0.6 & 73.9$\pm$ 0.5 &          \\
%                     Vehicle   & 44.9$\pm$ 0.6 & 61.5$\pm$ 0.4 & $\surd$  \\
%                     \bottomrule
%                 \end{tabular}
%             \end{sc}
%         \end{small}
%     \end{center}
%     \vskip -0.1in
% \end{table}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PROJECT OVERVIEW:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The goal of this project is for you to further explore a certain subject that we covered
% in class or a new subject related to it (based on some previous work). This is an open
% project in a sense that you will propose the problem you would like to work on and the
% solution you are planning use. Your main goal should be to explore the characteristics of
% your problem/solution under the reinforcement learning framework
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% REPORT INSTRUCTIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1. Introduction(5%)
%In this section you should discuss the question you are planning to investigate. Make
%sure to describe the problem clearly, and how you are planning to solve it. Also, clarify
%what your new contribution is (as opposed to previous works).
%2. Previous Work(5%)
%Discuss the previous work you are using as a starting point/reference. If you are
%working on a different domain include work that works on the same or similar domain.
%If you are using a new method, discuss other works which use similar methods. Note
%that you can use both academic papers in addition to online articles. Make sure to cite
%all sources.
%3. Domain Problem(5%)
%Describe the problem you are working on including the rules, goals, etc. Assume that
%the reader has not been introduced to this domain problem previously. Then describe
%how you frame it as an MDP, including: state space, action space and reward structure.
%If you are examining different options describe them all. Feel free to add diagrams to
%clarify.
%3
%4. Reinforcement Learning Methods(5%)
%Describe the reinforcement method/methods you are planning to use in this work. This
%should give the background for the methods. If you are comparing multiple ones, make
%sure to describe them all. Feel free to add diagrams and equations to clarify.
%5. Code Design(5%)
%Describe the general structure of your code including functions, classes, and data struc-
%tures used. If you are using code written by someone else make sure to cite it and
%emphasize the parts that you have written yourself.
%6. Results Analysis(40%)
%Present the results which answer your hypothesis questions. These should include dif-
%ferent graphs which clarify the answer and examine different aspects of your solution.
%Make sure to carefully describe how your experiments were conducted (how many runs,
%how it was initialized, etc. This section is the most important, and will be evaluated
%both on presentation and correctness.
%7. Conclusion(5%)
%What have you achieved in this project? What have you learned?
%An appendix describing which part of the project each team member was in charge of.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{GPT2PPO: Auto Regressive Proximal Policy Optimization\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Andrei Cozma}
\IEEEauthorblockA{\textit{Department of Electrical Engineering \& Computer Science} \\
\textit{University of Tennessee}\\
Knoxville, United States \\
acozma@vols.utk.edu}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Hunter Price}
\IEEEauthorblockA{\textit{Department of Electrical Engineering \& Computer Science} \\
\textit{University of Tennessee}\\
Knoxville, United States \\
hprice7@vols.utk.edu}
}

\maketitle


\begin{abstract}
    This document is a model and instructions for \LaTeX.
    This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
    or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
    Reinforcement Learning, PPO, GPT2
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Introduction(5%)
%In this section you should discuss the question you are planning to investigate. Make
%sure to describe the problem clearly, and how you are planning to solve it. Also, clarify
%what your new contribution is (as opposed to previous works).

\section{Introduction}
% \label{sec:introduction}
In this project we explore the use of transformers in the context of Reinforcement Learning. The majority of theoretical works 
assume that problems follow a Markovian process, which is not always the case. Some problems need the contxt of previous states and actions
to make an informed decision on the next decision. As a result, we propose an addition to the basic Proximal Policy Optimization (PPO) algorithm
by using the Generative Pre-trained Transformer 2 (GPT2) model as the encoder for the critic network. This will allow the critic network to
take into account the context of previous states and actions as well as apply attention to past states and actions that may be important. 
We will test this model on the LunarLander-v2 and Acrobot-v1 OpenAi Gym environments with discrete action spaces and compare it to the original PPO algorithm. 
Additionally we will test the model on BipedalWalker-v3 with continuous action spaces and compare it to the original PPO algorithm.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Previous Work(5%)
%Discuss the previous work you are using as a starting point/reference. If you are
%working on a different domain include work that works on the same or similar domain.
%If you are using a new method, discuss other works which use similar methods. Note
%that you can use both academic papers in addition to online articles. Make sure to cite
%all sources.

\section{Previous Work}
% \label{sec:previous-work}

% PPO citation - \cite{schulman2017proximal}
% Decision Transformer citation - \cite{chen2021decision}
% GPT2 citation - \cite{radford2019language}
% Wandb citation - \cite{wandb}
% Hugging face transformers citation - \cite{wolf-etal-2020-transformers}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Domain Problem(5%)
%Describe the problem you are working on including the rules, goals, etc. Assume that
%the reader has not been introduced to this domain problem previously. Then describe
%how you frame it as an MDP, including: state space, action space and reward structure.
%If you are examining different options describe them all. Feel free to add diagrams to
%clarify.
\section{Background}
% \label{sec:background}

All of the environments used in this project are from the OpenAI Gym library\cite{brockman2016openai}. The environments are described below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lunar Lander}

The Lunar Lander environment is a rocket trajectory optimization  
problem\footnote{OpenAI Gym Lunar Lander: \url{https://www.gymlibrary.dev/environments/box2d/lunar_lander}}
shown in Figure \ref{fig:lunar-lander-env}. The goal is to actuate the lander to the landing pad at coordinates (0,0) without crashing.
OpenAI Gym offers two versions of the environment: discrete or continuous.
In this work we only use the discrete version. 
The state space is a 8-dimensional vector containing the x and y positional coordinates of the agent, its x and y linear velocities,
its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.
The action space is a single discrete scalar with values ranging from 0 to 3. The values corresponspond to the following actions: 
do nothing, fire left orientation engine, fire main engine, fire right orientation engine.
The reward structure contains both positive and negative rewards. If the lander moves away from the landing pad, it gains a negative reward.
If the lander crashes, it receives an -100 reward. If it comes to rest, it receives an +100 reward. Each leg with ground contact is +10 points.
Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame. Firing the side engine is -0.03 points each frame.
The landers initial state is at the top center of the environment with a random intial force applied to its center. 
The episode ends if the lander crashes, goes outside of the viewport, or comes to a resting position.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/lunar-lander-env.png}}
    \caption{The Lunar Lander environment.}
    \label{fig:lunar-lander-env}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Acrobot}

Open AI Gym's implementation of the Acrobot 
environment\footnote{OpenAI Gym Acrobot: \url{https://www.gymlibrary.dev/environments/classic_control/acrobot}}
is based off the work of Sutton and Barto\cite{sutton2018reinforcement} shown in Figure \ref{fig:acrobot-env}.
The environment is a 2-link pendulum with only the second joint actuated with a discrete action space. The goal is to swing the end of the pendulum up to a given height. 
The state space is a 6-dimensional vector containing the sin and cos of the two joint angles and the joint angular velocities.
The action space is a single discrete scalar with values ranging from 0 to 5. The values corresponspond to the following actions:
apply +1, 0, or -1 torque to the actuated joint. 
The reward structure contains only negative rewards. At each timestep the agent receives a reward of -1 for each step that does not reach the goal.
If the goal is reached the agent receives a reward of 0. The episode ends if the agent reaches the goal height or if the episode exceeds the maximum number of timesteps.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/acrobot-env.png}}
    \caption{The Acrobot environment.}
    \label{fig:acrobot-env}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bipedal Walker}
%. There are no coordinates in the state vector.
The Bipedal Walker environment is a 2D simulation of a bipedal walker robot with 
4-joints\footnote{OpenAI Gym Acrobot: \url{https://www.gymlibrary.dev/environments/box2d/bipedal_walker}} 
shown in Figure \ref{bipedal-walker-env}. The goal is to keep the walker upright for as long as possible.
The state space is a 24-dimensional vector containing: 
the hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, 
legs contact with ground, and 10 lidar rangefinder measurements. Notably, there are no coordinates given in the state vector.
The action space is a 4 dimensional vector containing continuous values of each joints motor speed between -1 and 1.
The reward structure contains both positive and negative rewards. A positive reward is given for moving forward. If the agent falls it receives a reward of -100.
Applying motor torque costs a small amount of reward.
The agent's intial state is standing at the left of the terrain with the hull horizontal, with both legs in the same position with a slight knee angle. 
The episode terminates if the agent's hull makes contact with the ground or if the agent reaches the end of the terrain length.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/bipedal-walker-env.png}}
    \caption{The Bipedal Walker environment.}
    \label{bipedal-walker-env}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\label{sec:methodology}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Reinforcement Learning Methods(5%)
%Describe the reinforcement method/methods you are planning to use in this work. This
%should give the background for the methods. If you are comparing multiple ones, make
%sure to describe them all. Feel free to add diagrams and equations to clarify.

\subsection{Reinforcement Learning Methods}
\label{subsec:reinf-learning-methods}
% PPO, Transformers, GPT

% It will then feed the states and actions through linear layers to create state and action embeddings, and feed the 
% timesteps through an embedding layer which generates positional embeddings. It adds the positional embeddings to the state and action embeddings. 
% Then 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Code Design(5%)
%Describe the general structure of your code including functions, classes, and data struc-
%tures used. If you are using code written by someone else make sure to cite it and
%emphasize the parts that you have written yourself.

\section{Code Design}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Utilized Libraries}
In this work we chose to use pytorch as our main deep learning library. Pytorch is a popular deep learning library that is well documented and has a large community.
To help maintain the structure of the models we also used the Pytorch-Lightning library. 
Pytorch-Lightning is a high-level library that allows for easy training and testing of models.
Pytorch-Lightning also provides a module named Lightning-Bolts that contains many pre-built models and utilities.
For the GPT2 implementation we used the HuggingFace Transformers library.
We use the OpenAI Gym Library to provide the environments for our models to train and test on\cite{brockman2016openai}.
Finally, we use wandb to log the results of all of our experiments\cite{wandb}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{File Structure}

This works file structure is as follows. The main directory contains the \textbf{main\_train.py} and \textbf{main\_eval.py} files. 
These files are used to train and test the models respectively. The core code is contained within the \textbf{rllib} directory. 
The Within the \textbf{rllib/examples} contains an A2C and PPO baseline implementations that were taken from the Lighting-Bolts library, 
copied into our project. 

Within the \textbf{rllib} directory there are many files. The important files are the files that are primarily used 
are: \textbf{Model.py}, \textbf{GPT2PPO.py}, \textbf{CommonGPT2.py}, \textbf{CommonBase.py}, and \textbf{GPT2.py}.
These files will be discussed in more detail in the following sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training and Testing Tools}
To train a model we use the \textbf{main\_train.py} file. This file will train a model on a given environment and log the results.
To train a model we use the following command:
\begin{center}
    \texttt{python3 main\_train.py \textbackslash \\}
    \texttt{-m ppo\_gpt \textbackslash \\}
    \texttt{-e LunarLander-v2}
\end{center}

For the \textbf{main\_train.py} file we have the following additional command line arguments:
\begin{itemize}
    \item \texttt{-e} or \texttt{--env} - The Open AI Gym environment [Default: LunarLander-v2]
    \item \texttt{-m} or \texttt{--model\_name} - The name of the model to train [Default: ppo\_ex (PPO)]
    \item \texttt{-ne} or \texttt{--num\_epochs} - The number of epochs to train for [Default: 150]
    \item \texttt{--al\_check\_interval} - The itervale of epochs to run validation [Default: 5]
    \item \texttt{--wandb\_project} - The wandb project to log to [Default: rl\_project]
    \item \texttt{--wandb\_entity} - The wandb entity to log to [Default: ece517]
    \item \texttt{--seed} - The seed value for training [Default: 123]
\end{itemize}

The training command will train the model then save the trained model to the checkpoints directory with the following structure:
\begin{center}
    \texttt{checkpoints/\\<model\_name>/\\<env\_name>/\\<model\_hash>.pt}
\end{center}

To test a model (watch it interact in the environment) we use the \textbf{main\_eval.py}. This file will load a model from the checkpoints directory and 
run it in the environment.
To test a model we use the following command:
\begin{center}
    \texttt{python3 main\_eval.py -f <model\_checkpoint>}
\end{center}

For the \textbf{main\_eval.py} file we have the following additional command line arguments:
\begin{itemize}
    \item \texttt{-f} or \texttt{--file\_path} - The path to the model checkpoint
    \item \texttt{-ne} or \texttt{--num\_episodes} - The number of episodes to run [Default: 5]
    \item \texttt{--running\_rew\_len} - The length of rewards to store at once [Default: 50]
    \item \texttt{--seed} - The seed value for testing [Default: 123]
\end{itemize}

Both the \textbf{main\_train.py} and \textbf{main\_eval.py} files utilize the \textbf{rllib/Model.py} file. 
This file abstracts the training and evaluation process from the training and testing commands into a class named Model. 
It simply loads in the correct model and trains or tests it.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Models}

The baseline PPO model we use to compare against our results is in the \textbf{rllib/examples/PPOExample.py} file.
In it contains code taken from the Lighting-Bolts library with no modifications. We store this model locally to protect against 
any future api changes in the library. The file contains a class named PPO that inherits from the LightningModule class.
Our implementation is builds upon this class.

This works implementation resides at \textbf{rllib/GPT2PPO.py}. This file contains the GPT2PPO class which implements the same interface as
the reference PPO. Apart from the existing MLP, ActorContinuous, and ActorCategorical models used in the baseline PPO implementation that are  
given by the Lighting-Bolts library, we use 2 additonal models that are implmented in the \textbf{rllib/CommonBase.py} file and 
the \textbf{rllib/CommonGPT2.py} file.

The \textbf{rllib/CommonBase.py} file implements the CommonBase class. This class implements a simple Sequential model containing a 
Linear layer with a relu activation function followed by a Linear layer with no activation function. This class is used as the
the base of the ActorCategorical and ActorContinuous models. States are fed through this and immediately given to the actor. 

The \textbf{rllib/CommonGPT2.py} file implements the CommonGPT2 class. This class implements a usage of the GPT2 model which is stored 
in the \textbf{rllib/GPT2.py} file. The GPT2 model is taken from the Decision Transformer implementation\cite{chen2021decision}. In that work
the authors took the huggingface GPT2 model and removed the logic that implment positional embeddings\cite{radford2019language,wolf-etal-2020-transformers}.
Our CommonGPT2 shares large similarities with the Decision Transformer implementation\cite{chen2021decision}; however, we chose to only 
feed the states and actions to GPT2 rather than the states, actions, and rewards to go. As input, CommonGPT2 will take a history of timesteps, states, 
and actions as input of shape (batch\_size, sequence\_length,:). It will then return an embedding of the state and action. The embedding of the 
state is then fed to the critic and the critic predicts the value of the state. 

GPT2PPO has some differences from the baseline PPO in how the internals are structured. Because we are using a Transformer we need to keep track of the 
history of the states, actions, timesteps, and attention masks used by the model. We do this by storing each of the respective histories in a buffer
of a length equal to the context length of the GPT2 model. When the buffer is full, the oldest entry is removed and the new entry is added to the end 
of the buffer. The buffer is then fed to the GPT2 model and the most current state is fed through the CommonBase then the actor. The GPT2 model will 
then return an embedding of the state and action. The embedding of the state is given to the critic. All inputs are saved in a history array which is 
later used for training.
There are two main functions for data generation and training. The \textbf{generate\_trajectory\_samples} function is used to generate data for training.
It runs a configured number of timesteps then yields the model inputs, attention masks, actions, log probabilities, Q values, and Advantages for
all timesteps. The \textbf{training\_step} function takes a batch of data and calculates and returns the loss for the actor or critic. This loss is 
is used by the Pytorch-Lightning framework to update the actor or critic.

% TODO probably more detail can be added.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Results Analysis(40%)
%Present the results which answer your hypothesis questions. These should include dif-
%ferent graphs which clarify the answer and examine different aspects of your solution.
%Make sure to carefully describe how your experiments were conducted (how many runs,
%how it was initialized, etc. This section is the most important, and will be evaluated
%both on presentation and correctness.


\section{Results}
\label{sec:results}

In this work we compare the results of our GPT2PPO model against the baseline PPO model. 
We compare the results of both models on the Lunar Lander, Acrobot, and environments.
To keep all of our results consistent, when comparing models we ran each model with the
same hyperparemters for the same number of epochs.
The hyperparameters for both models are listed in Table \ref{tab:hyperparameters}.
We ran each model 3 times and averaged the results. 

\begin{table}[htbp]
    \caption{Model Hyperparameters}
    \begin{center}
    \begin{tabular}{|c|c|}
    \hline
        \multicolumn{2}{|c|}{\textbf{Hyperparameters}} \\
    \hline
    \textbf{Param} & \textbf{\textit{Value}} \\
    \hline
        Environment Seed & 123 \\
        Epochs & 150 \\
        Gamma & 0.99 \\
        Lambda & 0.95 \\
        Batch Size & 128 \\
        Actor Learning Rate & 3e-4 \\
        Critic Learning Rate & 1e-3 \\
        Max Episode Length & 500 \\
        Steps Per Epoch & 2048 \\
        Training Steps Per Epoch & 5 \\
        Clip Ratio & 0.2 \\
        Hidden Size & 64 \\
        Context Length* & 64 \\
    \hline
    \multicolumn{2}{l}{*Context Length only used for GPT2PPO.}
    \end{tabular}
    \label{tab:hyperparameters}
    \end{center}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lunar Lander}

%% PPO vs GPT2PPO - Comparison
\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/lunar-lander-loss_critic-model.png}}
    \caption{Lunar Lander - PPO vs GPT2PPO - Critic Loss}
    \label{lunar-lander-loss_critic-model}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/lunar-lander-loss_actor-model.png}}
    \caption{Lunar Lander - PPO vs GPT2PPO - Actor Loss}
    \label{lunar-lander-loss_actor-model}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/lunar-lander-avg_ep_reward-model.png}}
    \caption{Lunar Lander - PPO vs GPT2PPO - Average Episode Reward}
    \label{lunar-lander-avg_ep_reward-model}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/lunar-lander-avg_ep_len-model.png}}
    \caption{Lunar Lander - PPO vs GPT2PPO - Average Episode Length}
    \label{lunar-lander-avg_ep_len-model}
\end{figure}


%% GPT2PPO - Different Context Lengths
% Context lengths of [25,50,75]
\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/lunar-lander-loss_critic-ctx_len.png}}
    \caption{Lunar Lander - GPT2PPO Context Length - Critic Loss}
    \label{lunar-lander-loss_critic-ctx_len}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/lunar-lander-loss_actor-ctx_len.png}}
    \caption{Lunar Lander - GPT2PPO Context Length - Actor Loss}
    \label{lunar-lander-loss_actor-ctx_len}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/lunar-lander-avg_ep_reward-ctx_len.png}}
    \caption{Lunar Lander - GPT2PPO Context Length - Average Episode Reward}
    \label{lunar-lander-avg_ep_reward-ctx_len}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/lunar-lander-avg_ep_len-ctx_len.png}}
    \caption{Lunar Lander - GPT2PPO Context Length - Average Episode Length}
    \label{lunar-lander-avg_ep_len-ctx_len}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Acrobot}

%% PPO vs GPT2PPO - Comparison
\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/acrobot-loss_critic-model.png}}
    \caption{Acrobot - PPO vs GPT2PPO - Critic Loss}
    \label{acrobot-loss_critic-model}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/acrobot-loss_actor-model.png}}
    \caption{Acrobot - PPO vs GPT2PPO - Actor Loss}
    \label{acrobot-loss_actor-model}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/acrobot-avg_ep_reward-model.png}}
    \caption{Acrobot - PPO vs GPT2PPO - Average Episode Reward}
    \label{acrobot-avg_ep_reward-model}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/acrobot-avg_ep_len-model.png}}
    \caption{Acrobot - PPO vs GPT2PPO - Average Episode Length}
    \label{acrobot-avg_ep_len-model}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bipedal Walker}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/bipedal-walker-loss_critic-model.png}}
    \caption{Bipedal Walker - PPO vs GPT2PPO - Critic Loss}
    \label{bipedal-walker-loss_critic-model}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/bipedal-walker-loss_actor-model.png}}
    \caption{Bipedal Walker - PPO vs GPT2PPO - Actor Loss}
    \label{bipedal-walker-loss_actor-model}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/bipedal-walker-avg_ep_reward-model.png}}
    \caption{Bipedal Walker - PPO vs GPT2PPO - Average Episode Reward}
    \label{bipedal-walker-avg_ep_reward-model}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{./img/bipedal-walker-avg_ep_len-model.png}}
    \caption{Bipedal Walker - PPO vs GPT2PPO - Average Episode Length}
    \label{bipedal-walker-avg_ep_len-model}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Conclusion(5%)
%What have you achieved in this project? What have you learned?

\section{Conclusion}
% \label{sec:conclusion}
% \lipsum[2-4]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References
\bibliographystyle{IEEEtran}
\bibliography{main}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% An appendix describing which part of the project each team member was in charge of.
\appendix
\subsection{Team Member Contributions}

\textbf{Andrei Cozma}. text here.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Hunter Price}. text here.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
