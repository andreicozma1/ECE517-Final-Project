@article{BEAL2001105,
title = {Temporal difference learning applied to game playing and the results of application to shogi},
journal = {Theoretical Computer Science},
volume = {252},
number = {1},
pages = {105-119},
year = {2001},
note = {CG'98},
issn = {0304-3975},
doi = {https://doi.org/10.1016/S0304-3975(00)00078-5},
url = {https://www.sciencedirect.com/science/article/pii/S0304397500000785},
author = {Donald F. Beal and Martin C. Smith},
keywords = {Learning, Temporal difference, Minimax, Search, Shogi},
abstract = {This paper describes the application of temporal difference (TD) learning to minimax searches in general, and presents results from shogi. TD learning is used to adjust the weights for evaluation features over the course of a series of games, starting from arbitrary initial values. For some games, to obtain weights accurate enough for high-performance play will require the TD learning phase to make use of minimax searches. A theoretical description of TD applied to minimax search is given, and we discuss how the theoretical characteristics of the method interact with practical considerations. These include the depth of search appropriate for successful learning and the use of self-play to enable the algorithm to be independent of human knowledge. We then report on experiments that obtained values for use in shogi-playing programs. Unlike chess, shogi has no generally agreed standardized set of values for pieces, so there is more need for machine learning. We compare our machine-learnt values, obtained without any human knowledge input, with hand-crafted values. TD learning was successful in obtaining values that performed well in matches against hand-crafted values.}
}
